# AI Analytics Assistant

Natural Language to SQL analytics powered by LLMs (OpenAI GPT-4 or Anthropic Claude).

## Features

- **Natural Language Queries**: Ask questions about your data in plain English
- **Smart SQL Generation**: Converts questions to optimized PostgreSQL queries
- **Auto-Visualization**: Automatically generates appropriate charts for results
- **Conversation Context**: Maintains context for follow-up questions
- **Safety First**: SQL validation prevents dangerous queries

## Setup

### 1. Get an OpenAI API Key

1. Go to [OpenAI Platform](https://platform.openai.com/api-keys)
2. Create an account or sign in
3. Click **"Create new secret key"**
4. Copy the key (starts with `sk-`)

### 2. Install Dependencies

```bash
cd streamlit
source venv/bin/activate
pip install -r requirements.txt
```

### 3. Configure Your API Key

Create a `.env` file in the `streamlit/` directory:

```bash
# Create .env file with your API key
cat > .env << 'EOF'
OPENAI_API_KEY=sk-proj-your-actual-key-here
EOF
```

Or manually create `streamlit/.env`:

```env
# Required: Your OpenAI API key
OPENAI_API_KEY=sk-proj-xxxxxxxxxxxxxxxxxxxxxxxx

# Optional: Use a different model (default: gpt-4o)
# OPENAI_MODEL=gpt-4o-mini  # Cheaper option
```

> ‚ö†Ô∏è **Security:** The `.env` file is in `.gitignore` and will NOT be committed to Git.

**Alternative: Anthropic Claude**
```env
ANTHROPIC_API_KEY=sk-ant-your-anthropic-api-key
```

You can also enter the API key directly in the Streamlit sidebar if not using `.env`.

### 3. Run the Dashboard

```bash
streamlit run app.py
```

Navigate to "ü§ñ AI Assistant" in the sidebar.

## Architecture

```
ai/
‚îú‚îÄ‚îÄ __init__.py           # Module exports
‚îú‚îÄ‚îÄ schema_context.py     # Loads semantic context from dim_metric
‚îú‚îÄ‚îÄ sql_generator.py      # LLM-based SQL generation
‚îú‚îÄ‚îÄ sql_validator.py      # SQL safety validation
‚îú‚îÄ‚îÄ visualizer.py         # Auto-visualization for results
‚îî‚îÄ‚îÄ README.md             # This file
```

## How It Works

1. **Schema Context**: Loads metric definitions from `dim_metric` and table schemas
2. **Prompt Engineering**: Builds a comprehensive system prompt with:
   - Available tables and their columns
   - Metric definitions and business meanings
   - Few-shot SQL examples
3. **SQL Generation**: LLM converts natural language to SQL
4. **Validation**: SQL is validated for safety (SELECT only, no injection)
5. **Execution**: Query runs against PostgreSQL
6. **Visualization**: Results are analyzed and visualized appropriately

## Example Questions

- "What is our total revenue by territory?"
- "Show me the top 10 customers by lifetime value"
- "What is the monthly revenue trend for 2014?"
- "Which products have the highest profit margin?"
- "Show me customers at risk of churning"
- "What is the average order value by customer segment?"

## Security

The `SQLValidator` ensures:
- Only SELECT queries are executed
- No DDL (CREATE, DROP, ALTER, etc.)
- No DML (INSERT, UPDATE, DELETE)
- Whitelist of allowed tables
- Automatic LIMIT clause (prevents huge result sets)
- No SQL injection patterns

## Customization

### Adding New Tables (Automatic Sync)

Tables are **automatically synced** with dbt models! When you add a new model:

1. Create the dbt model (e.g., `mart_new_analytics.sql`)
2. Add schema definition to `_schema.yml`
3. Run dbt:
   ```bash
   cd ../../dbt
   ./run_dbt.sh run
   ```

This auto-generates:
- `allowed_tables.json` - SQL validator whitelist (this directory)
- `schema_ai.md` - LLM context (`dbt/models/`)

**No manual editing required!** ‚ú®

### Architecture Files

```
ai/
‚îú‚îÄ‚îÄ __init__.py           # Module exports
‚îú‚îÄ‚îÄ schema_context.py     # Loads semantic context (reads allowed_tables.json)
‚îú‚îÄ‚îÄ sql_generator.py      # LLM-based SQL generation
‚îú‚îÄ‚îÄ sql_validator.py      # SQL safety validation (reads allowed_tables.json)
‚îú‚îÄ‚îÄ visualizer.py         # Auto-visualization for results
‚îú‚îÄ‚îÄ allowed_tables.json   # Auto-generated by dbt run (DO NOT EDIT)
‚îî‚îÄ‚îÄ README.md             # This file
```

### Adding Few-Shot Examples

Edit `schema_context.py`:

```python
def get_example_queries(self) -> list:
    return [
        ("Your question", "SELECT ..."),
        ...
    ]
```

### Changing LLM Model

Set environment variables in `.env`:
```env
# OpenAI (default)
OPENAI_API_KEY=sk-proj-your-key
OPENAI_MODEL=gpt-4o-mini  # Cheaper option (default: gpt-4o)

# Or Anthropic
ANTHROPIC_API_KEY=sk-ant-your-key
ANTHROPIC_MODEL=claude-3-haiku-20240307  # Cheaper option
```

### Manual Table Override (Not Recommended)

If you need to manually add tables without running dbt, edit `allowed_tables.json`:

```json
{
  "allowed_tables": [
    "mart_sales",
    "your_custom_table"
  ]
}
```

‚ö†Ô∏è **Warning:** This file is overwritten on every `dbt run`. Add your model to dbt instead.
